# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_matching_columns.ipynb.

# %% auto 0
__all__ = ['createColsList', 'find_identical_cols', 'scoreConnections', 'formatForD3', 'find_similar_cols', 'plot_network']

# %% ../nbs/06_matching_columns.ipynb 3
import pandas as pd
import plotly.express as px
from sklearn.decomposition import PCA
import re
import ast

# %% ../nbs/06_matching_columns.ipynb 5
def createColsList(metadata_df):
    """
    Loop through the metadata dataframe and extract the columns and the headings
    (keys) of the unique parameters. This ensures datasets with no data in the
    'Columns' column still have the unique parameters column to go off.
    """
    c = 0
    faults = 0
    cols_list = []
    for row in metadata_df.loc[:, ['Columns', 'Unique_parameters']].values:
        temp_col_list = []
        # print(c)
        # c+=1
    
        # Column titles
        if type(row[0]) == str:
            temp_col_list += ast.literal_eval(row[0])
        else:
            temp_col_list.append(row[0])
    
        # Unique parameters
        if type(row[1]) == float:
            cols_list.append(temp_col_list)
            # print(str(row[1]))
            faults += 1
        else:
            try:
                temp_col_list += (list(ast.literal_eval(row[1]).keys()))
                cols_list.append(temp_col_list)
            except:
                cols_list.append(temp_col_list)
                faults += 1
                continue
    
    # Removing extra text within brackets, converting to lowercase and removing duplicates
    for i in range(len(cols_list)):
        for j in range(len(cols_list[i])):
            cols_list[i][j] = re.sub(r"\([^()]*\)", "", str(cols_list[i][j]))
            cols_list[i][j] = cols_list[i][j].lower()
        cols_list[i] = list(set(cols_list[i]))

    return cols_list

# %% ../nbs/06_matching_columns.ipynb 8
def find_identical_cols(cols_list, compare_idx):
    """
    Function to find the column titles of datasets that are identical to those
    in the dataset to be compared with.


    The index of the list must be used
    - To find the index of a named dataset we can simply search for the name in 
    the metadata_df and find its corresponding index
    """

    dataset_to_compare = compare_idx

    comp_cols = cols_list[dataset_to_compare]
    comp_cols = [s.strip() for s in comp_cols]
    updated_cols_list = cols_list[:]
    updated_cols_list.pop(dataset_to_compare)

    identical_cols = []
    pattern = re.compile(r"v\d+_\d+") # we want to remove strings in the form vX_Y
    for cl in updated_cols_list:

        # Clean list
        temp_cl = [s.strip() for s in cl]
        temp_cl = [s for s in temp_cl if not pattern.match(s)]

        # Add a list of strings that match from both comp_cols and temp_cl
        identical_cols.append(list(set(comp_cols) & set(temp_cl)))

    return identical_cols

# %% ../nbs/06_matching_columns.ipynb 11
def scoreConnections(cols_list, method="", rare=True):
    """Giving a score to each column title based on its rarity"""
    # Loop through every datasets list of unique columns
    score_dict = {}
    # temp_cols = []
    for comp_dataset in range(len(cols_list)):
        # Loop through the unique columns and count the number of times each appears
        for col in cols_list[comp_dataset]:
            # temp_cols.append(col)
            if col in score_dict:
                score_dict[col.strip()] += 1
            else:
                score_dict[col.strip()] = 1
    
    s = score_dict.values()

    if (method == "alt"):
        s = [5/freq for freq in s] # alt method of weighting connections (most connections end up being rounded to 0 leading to around 300 datasets never appearing)
    else:
        # Original method
        s = [min(freq-1, 20)/10 for freq in s] # force values to be between 0 and 2
        s = [round(-1*(x-3), 1) + 0 for x in s] # reversing the order (we want to prioritise less common columns)(between 1 and 2)
    
    score_dict = {list(score_dict.keys())[i]: s[i] for i in range(len(score_dict))} # reassigning values

    """
    Constructing the network dataframe using scored identical column values
    
    Each column title is given a value based on it's rarity.
    The weight of a connection is the sum of the values of the identical columns 
    between the two datasets; capping the max weight to 10.
    """
    titles = metadata_df.loc[:, 'Title']
    score_net_df = pd.DataFrame(columns=['Source', 'Target', 'weight'])
    cs_list = cols_list
    
    while(len(titles) > 1):
        # print(len(titles))
        compare_dataset = titles[0]
        titles = list(titles[1:]) # Ignore the dataset being compared to the others
    
        scores = []
        identical_cols = find_identical_cols(cs_list, # List of lists containing the unique columns from each dataset
                                             0) # The first dataset in the list
        cs_list = cs_list[1:]
    
        datasets_i_cols = [] # The title of the dataset which has at least one identical column when compared with the compare_dataset
        idx = 0
        for i_col_list in identical_cols:
            temp_score = round(sum([score_dict[col] for col in i_col_list]), 1)
            if (temp_score > 0) & (temp_score <= 10):
                datasets_i_cols.append(titles[idx])
                scores.append(temp_score)
            elif temp_score > 10:
                datasets_i_cols.append(titles[idx])
                scores.append(10)
            
            idx += 1
    
        source_labels = [compare_dataset] * len(scores)
        temp_df = pd.DataFrame({'Source': source_labels, 'Target': datasets_i_cols, 'weight': scores})
        score_net_df = pd.concat([score_net_df, temp_df]).reset_index(drop=True)

    if rare:
        # Creating a list of datasets that aren't well connected and updating their weights to make them
        # more likely to appear
        rare_connections = score_net_df.Target.value_counts().index[score_net_df.Target.value_counts() <= 20]
        score_net_df.loc[score_net_df.Target.isin(rare_connections), 'weight'] = 10

    return score_net_df

# %% ../nbs/06_matching_columns.ipynb 12
def formatForD3(scoredConnections, cutoff=0, save=False, saveName=""):
    """
    Converting Source and Target to integers for use in a D3 network diagram.
    To save as a JSON, set save to True which will automatically save to the 
    correct folder.
    """
    id_df = pd.read_json('data/datasets_titles_ids.json')
    
    temp_sNet_df = scoredConnections
    
    # Creating the replacement dictionary
    replacement_dict = id_df.set_index('Title')['id'].to_dict()
    
    # Replacing 'Source' and 'Target' with corresponding 'id'
    temp_sNet_df['Source'] = temp_sNet_df['Source'].map(replacement_dict)
    temp_sNet_df['Target'] = temp_sNet_df['Target'].map(replacement_dict)

    if cutoff > 0:
        temp_sNet_df = temp_sNet_df.loc[temp_sNet_df.weight >= cutoff].reset_index(drop=True)

    if save and (saveName != ""):
        fileName = r'data/' + saveName + '.json'
        temp_sNet_df.to_json(fileName, orient="columns", force_ascii=False)
    
    return temp_sNet_df

# %% ../nbs/06_matching_columns.ipynb 17
def find_similar_cols(cols_list, compare_idx):
    """
    Function to find the column titles of datasets that are similar to those
    in the dataset to be compared with.


    The index of the list must be used
    - To find the index of a named dataset we can simply search for the name in 
    the metadata_df and find its corresponding index
    """
    similar_columns = [] # stores the lists of similar columns between the dataset being compared and every other dataset
    
    compare_cols = cols_list[compare_idx]
    other_cols_list = cols_list[:]
    other_cols_list.pop(compare_idx)
    
    for i in range(len(other_cols_list)): # looping over the number of datasets
        temp_sim_cls = [] # stores the similar columns between the dataset being compared and another dataset
        for col in compare_cols: # looping over the unique cols for the dataset being compared
            for c in other_cols_list[i]:
                if re.match(r"v\d+_\d+", col): # ignore if it has the form vX_Y
                    continue
                if (textdistance.jaro_winkler(col, c) > 0.9) & (textdistance.jaro_winkler(col, c) != 1):
                    temp_sim_cls.append(c)

        similar_columns.append(temp_sim_cls)
    
    return similar_columns
            

# %% ../nbs/06_matching_columns.ipynb 25
from math import sqrt

def plot_network(G, show_edges=True):
    """ Plot network diagram """
    node_sizes = [min(len(list(G.edges(node)))*0.2+6, 12) for node in G.nodes()]
    
    # Set the positions of the nodes
    pos = nx.spring_layout(G, k=1.2/sqrt(len(node_sizes)), seed=42)
    
    if show_edges:
        # Create a list to store the edge traces
        edge_traces = []

        # Create an edge trace for each edge with varying width
        for u, v, d in G.edges(data=True):
            x0, y0 = pos[u]
            x1, y1 = pos[v]
            width = min(d['weight']*2, 4)

            edge_trace = go.Scatter(
                x=[x0, x1, None],
                y=[y0, y1, None],
                line=dict(width=width, color='gray'),
                hoverinfo='none',
                mode='lines'
            )

            edge_traces.append(edge_trace)
    
    # Create a node trace
    node_trace = go.Scatter(
        x=[pos[node][0] for node in G.nodes()],
        y=[pos[node][1] for node in G.nodes()],
        text=[node for node in G.nodes()],
        mode='markers',
        hoverinfo='text',
        marker=dict(
            color=node_sizes,
            size=node_sizes # The size of the node depends on the number of edges connected to it
        )
    )
    
    if show_edges:
        data = edge_traces + [node_trace]
    else:
        data = node_trace
    
    # Create the network graph figure
    fig = go.Figure(data=data,
                    layout=go.Layout(
                        title='',
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20, l=5, r=5, t=40),
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
                    )
                )
    
    # Display the network graph
    fig.layout.height = 750
    fig.show()
    
    return fig
