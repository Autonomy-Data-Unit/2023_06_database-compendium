# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/00_ONS_scraper_functions.ipynb.

# %% auto 0
__all__ = ['get_ONS_datasets_titles_descriptions', 'get_ONS_datasets_urls', 'find_ONS_cols', 'get_ONS_long_description']

# %% ../../nbs/00_ONS_scraper_functions.ipynb 4
import requests
import pandas as pd
from io import StringIO
from bs4 import BeautifulSoup
import re

# %% ../../nbs/00_ONS_scraper_functions.ipynb 5
def get_ONS_datasets_titles_descriptions():
    """Load ONS api and loop through all available datasets 
    collecting their titles and a short description."""
    api_url = "https://api.beta.ons.gov.uk/v1/datasets"
    offset = 0
    max = 500
    titles = []
    descriptions = []

    if requests.get(api_url).status_code == 200:
        while len(titles) < max:
            response = requests.get(api_url, params={"offset": offset})

            try:
                result = response.json()  # load json file
            except:
                continue
                
            r = result['items']
            for data in r:
                titles.append(data['title'])  # add dataset titles to a list
                descriptions.append(data['description']) # add dataset descriptions to a list

            offset += result['count']  # offset to avoid datasets already seen

            if result['count'] == 0:
                break

    else:
        titles = "Error: " + str(requests.get(api_url).status_code)
    
    return titles, descriptions

# %% ../../nbs/00_ONS_scraper_functions.ipynb 7
def get_ONS_datasets_urls():
    """
    Loads ONS api and loops through collecting their urls (that can be used to download a csv file of each one).
    """

    api_url = "https://api.beta.ons.gov.uk/v1/datasets"
    offset = 0
    max = 500
    datasets_urls = []

    while len(datasets_urls) < max:
        response = requests.get(api_url, params={"offset": offset})

        try:
            result = response.json()
        except:
            continue

        r = result['items']
        for row in r:
            edition = row.get("links").get("latest_version").get("href")
            datasets_urls.append(edition)

        offset += result['count']  # offset to avoid datasets already seen

        if result['count'] == 0:
            break

    return datasets_urls

# %% ../../nbs/00_ONS_scraper_functions.ipynb 9
def find_ONS_cols(url):
    "Finding the csv download link for a specific dataset."
    r = requests.get(url)

    if r.status_code == 200:
        test = r.json()
    
        if test.get('downloads'):
            temp_url = test.get("downloads").get("csv").get("href")
    
            # Has to be done like this to avoid HTTP 403 Error
            # Solution found at: https://datascience.stackexchange.com/questions/49751/read-csv-file-directly-from-url-how-to-fix-a-403-forbidden-error
            csv_url = requests.get(temp_url).text
            temp_df = pd.read_csv(StringIO(csv_url), dtype='string', on_bad_lines='skip')
            col_headings = temp_df.columns
            col_headings = col_headings.to_list()
        else:
            col_headings = float('nan')  # This means the link did not have a csv file href
    else:
        col_headings = float('nan')

    return col_headings

# %% ../../nbs/00_ONS_scraper_functions.ipynb 11
def get_ONS_long_description():
    """
    Getting a long description from the Quality and Methodology (QMI) page
    for all datasets available via the ONS api.
    """
    api_url = "https://api.beta.ons.gov.uk/v1/datasets"
    description_L = []
    MAX_RETRIES = 100

    for i in range(MAX_RETRIES):
        try:
            # Getting the qmi (Quality and Methodology Information) url
            response = requests.get(api_url, params={"limit": 1000})
            items = response.json()['items']
            break
        except:
            continue

    
    i = 0
    for item in items:

        try:
            qmi_url = item['qmi']['href']

            # Reading the HTML from the page
            response2 = requests.get(qmi_url).text
            soup = BeautifulSoup(response2, 'html.parser')

            """
            Searching for all text content in <p> elements. Removing any elements 
            contained within the <p> elements. Removing strings longer than 35 
            characters (to try get only descriptive content) and cleaning the content
            by removing \n and ' characters as well as any double spaces and the 
            square brackets surrounding each string.
            """
            temp_desc = ''
            for text in soup('p')[4:-7]:
                if text.contents != '':
                    x = str(text.contents)
                    y = re.sub("\<.*?\>", "", str(x))
                    y = re.sub(r"\s+", " ", y)
                    y = y.replace('\\n', '').replace('\'', '').replace(',', '')\
                    .replace('  ', ' ').replace('\\xa0', ' ')
                    if len(y) > 35:
                        temp_desc = temp_desc + y[1:len(y)-1]
        except:
            description_L.append('')
            continue

        description_L.append(temp_desc)

    return description_L
