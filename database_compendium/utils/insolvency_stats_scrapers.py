# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_insolvency_stats_scrapers.ipynb.

# %% auto 0
__all__ = ['get_insolvency_stats', 'get_insolvency_unique_column_vals', 'get_mis_last_updated', 'get_mis_description']

# %% ../../nbs/02_insolvency_stats_scrapers.ipynb 4
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import requests

# %% ../../nbs/02_insolvency_stats_scrapers.ipynb 5
def get_insolvency_stats():
    """
    Find the most up to date monthly insolvency statistics and fetch the 
    url so it can be used to collect the .xlsx file.

    Open the webpage and read the HTML.
    Find the .xlsx file by searching through the <a> elements.
    (we know it's in an <a> element as this is how links are defined in HTML)

    Returns a dictionary where the keys are the sheet names and the values are
    dataframes containing the sheet data.
    """
    base_url = "https://www.gov.uk"
    url = base_url + "/government/collections/monthly-insolvency-statistics"

    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        uls = soup.find("ul", {"class": "gem-c-document-list"})

        insolvency_stats_url = uls.findChildren("a")[0]['href'] # link for first item in list (most recent stats)
        insolvency_stats_url = base_url + insolvency_stats_url

        response = requests.get(insolvency_stats_url)

        # Opening the relevant file
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            for a in soup.find_all("a", {"class": "govuk-link"}):
                if 'xlsx' in a['href']:
                    r = requests.get(a['href'])
                    file = pd.ExcelFile(r.content)

        # Reading the methods and notes from sheets in the document to create a long description
        for sheet in file.sheet_names:
            if 'Method' in sheet:
                MaQ = file.parse(sheet)[1:]
            if 'Note' in sheet:
                notes = file.parse(sheet).iloc[1:,:2]

        MaQ = '\n'.join(MaQ.iloc[:, 0])
        notes.iloc[:, 0] = '[' + notes.iloc[:, 0] + ']'
        notes = notes.iloc[: ,0] + ' ' + notes.iloc[:, 1]
        notes = '\n'.join(notes)
        long_description = MaQ + ' \n' + notes

        # Extracting the sheets that contain the actual data
        dfs = {}

        for sheet in file.sheet_names:
            if 'Table' in sheet:
                title = file.parse(sheet).keys()[0]
                dfs[title] = file.parse(sheet) # save as df in dictionary

        for table in dfs:
            skip_rows = np.argmax(~dfs[table].iloc[:, 2].isnull()) # find the number of redundant rows
            dfs[table].columns = dfs[table].iloc[skip_rows, :]  # Replace columns with correct values
            dfs[table] = dfs[table][skip_rows+1:]   # ignore the empty rows
        
    return dfs, long_description

# %% ../../nbs/02_insolvency_stats_scrapers.ipynb 8
def get_insolvency_unique_column_vals(data):
    "Given a dataframe, this function finds the unique column values"
    col_data = {}
    data = data.reset_index(drop=True)

    for col in data.columns:

        if type(data.loc[:, col][0]) == str: # Check for string data type
            if not data.loc[:, col][0].replace('.','', 1).isdigit(): # if the data is a string ensure that it isn't numeric
                col_data[col] = list(data.loc[:, col].unique())

    return col_data

# %% ../../nbs/02_insolvency_stats_scrapers.ipynb 9
def get_mis_last_updated():
    "Gets the publish date of the most up to date insolvency data"
    base_url = "https://www.gov.uk"
    url = base_url + "/government/collections/monthly-insolvency-statistics"

    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        last_updated_element = soup.find(text="Last updated")
        last_updated = last_updated_element.find_next()
        last_updated = last_updated.text.strip().split('\n')[0]
    else:
        last_updated = float('NaN')

    return last_updated

# %% ../../nbs/02_insolvency_stats_scrapers.ipynb 11
def get_mis_description():
    "Gets a short description of the insolvency statistics dataset"
    base_url = "https://www.gov.uk"
    url = base_url + "/government/collections/monthly-insolvency-statistics"

    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        # Find the <p> element with a class containing the word "lead"
        lead_paragraph = soup.select("p[class*=lead]")
    else:
        lead_paragraph = float('NaN')

    lead_paragraph = lead_paragraph[0].text

    return lead_paragraph
