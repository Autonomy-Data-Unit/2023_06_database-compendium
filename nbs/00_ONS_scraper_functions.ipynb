{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Data from the ONS API  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automatically download datasets metadata from the ONS\n",
    "- Get the titles of each dataset\n",
    "- Find a description\n",
    "- Get the column titles\n",
    "\n",
    "Author: Rowan Trickett    \n",
    "Date: 13/04/2023    \n",
    "Last updated: 24/04/2023  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.ONS_scraper_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_ONS_datasets_titles_descriptions():\n",
    "    \"\"\"Load ONS api and loop through all available datasets \n",
    "    collecting their titles and a short description.\"\"\"\n",
    "    api_url = \"https://api.beta.ons.gov.uk/v1/datasets\"\n",
    "    offset = 0\n",
    "    max = 500\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "\n",
    "    if requests.get(api_url).status_code == 200:\n",
    "        while len(titles) < max:\n",
    "            response = requests.get(api_url, params={\"offset\": offset})\n",
    "\n",
    "            try:\n",
    "                result = response.json()  # load json file\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            r = result['items']\n",
    "            for data in r:\n",
    "                titles.append(data['title'])  # add dataset titles to a list\n",
    "                descriptions.append(data['description']) # add dataset descriptions to a list\n",
    "\n",
    "            offset += result['count']  # offset to avoid datasets already seen\n",
    "\n",
    "            if result['count'] == 0:\n",
    "                break\n",
    "\n",
    "    else:\n",
    "        titles = \"Error: \" + str(requests.get(api_url).status_code)\n",
    "    \n",
    "    return titles, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Quarterly personal well-being estimates\n",
      "Description: Seasonally and non seasonally-adjusted quarterly estimates of life satisfaction, feeling that the things done in life are worthwhile, happiness and anxiety in the UK.\n"
     ]
    }
   ],
   "source": [
    "titles, descriptions = get_ONS_datasets_titles_descriptions()\n",
    "print('Title: ' + titles[0] + '\\n' + 'Description: ' + descriptions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_ONS_datasets_urls():\n",
    "    \"\"\"\n",
    "    Loads ONS api and loops through collecting their urls (that can be used to download a csv file of each one).\n",
    "    \"\"\"\n",
    "\n",
    "    api_url = \"https://api.beta.ons.gov.uk/v1/datasets\"\n",
    "    offset = 0\n",
    "    max = 500\n",
    "    datasets_urls = []\n",
    "\n",
    "    while len(datasets_urls) < max:\n",
    "        response = requests.get(api_url, params={\"offset\": offset})\n",
    "\n",
    "        try:\n",
    "            result = response.json()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        r = result['items']\n",
    "        for row in r:\n",
    "            edition = row.get(\"links\").get(\"latest_version\").get(\"href\")\n",
    "            datasets_urls.append(edition)\n",
    "\n",
    "        offset += result['count']  # offset to avoid datasets already seen\n",
    "\n",
    "        if result['count'] == 0:\n",
    "            break\n",
    "\n",
    "    return datasets_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.beta.ons.gov.uk/v1/datasets/wellbeing-quarterly/editions/time-series/versions/7'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The url for the first dataset in the list\n",
    "get_ONS_datasets_urls()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_ONS_cols(url):\n",
    "    \"Finding the csv download link for a specific dataset.\"\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        test = r.json()\n",
    "    \n",
    "        if test.get('downloads'):\n",
    "            temp_url = test.get(\"downloads\").get(\"csv\").get(\"href\")\n",
    "    \n",
    "            # Has to be done like this to avoid HTTP 403 Error\n",
    "            # Solution found at: https://datascience.stackexchange.com/questions/49751/read-csv-file-directly-from-url-how-to-fix-a-403-forbidden-error\n",
    "            csv_url = requests.get(temp_url).text\n",
    "            temp_df = pd.read_csv(StringIO(csv_url), dtype='string', on_bad_lines='skip')\n",
    "            col_headings = temp_df.columns\n",
    "            col_headings = col_headings.to_list()\n",
    "        else:\n",
    "            col_headings = float('nan')  # This means the link did not have a csv file href\n",
    "    else:\n",
    "        col_headings = float('nan')\n",
    "\n",
    "    return col_headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.beta.ons.gov.uk/v1/datasets/wellbeing-quarterly/editions/time-series/versions/7'\n",
    "for url in get_ONS_datasets_urls():\n",
    "    find_ONS_cols(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_ONS_long_description():\n",
    "    \"\"\"\n",
    "    Getting a long description from the Quality and Methodology (QMI) page\n",
    "    for all datasets available via the ONS api.\n",
    "    \"\"\"\n",
    "    api_url = \"https://api.beta.ons.gov.uk/v1/datasets\"\n",
    "    description_L = []\n",
    "    MAX_RETRIES = 100\n",
    "\n",
    "    for i in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Getting the qmi (Quality and Methodology Information) url\n",
    "            response = requests.get(api_url, params={\"limit\": 1000})\n",
    "            items = response.json()['items']\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    i = 0\n",
    "    for item in items:\n",
    "\n",
    "        try:\n",
    "            qmi_url = item['qmi']['href']\n",
    "\n",
    "            # Reading the HTML from the page\n",
    "            response2 = requests.get(qmi_url).text\n",
    "            soup = BeautifulSoup(response2, 'html.parser')\n",
    "\n",
    "            \"\"\"\n",
    "            Searching for all text content in <p> elements. Removing any elements \n",
    "            contained within the <p> elements. Removing strings longer than 35 \n",
    "            characters (to try get only descriptive content) and cleaning the content\n",
    "            by removing \\n and ' characters as well as any double spaces and the \n",
    "            square brackets surrounding each string.\n",
    "            \"\"\"\n",
    "            temp_desc = ''\n",
    "            for text in soup('p')[4:-7]:\n",
    "                if text.contents != '':\n",
    "                    x = str(text.contents)\n",
    "                    y = re.sub(\"\\<.*?\\>\", \"\", str(x))\n",
    "                    y = re.sub(r\"\\s+\", \" \", y)\n",
    "                    y = y.replace('\\\\n', '').replace('\\'', '').replace(',', '')\\\n",
    "                    .replace('  ', ' ').replace('\\\\xa0', ' ')\n",
    "                    if len(y) > 35:\n",
    "                        temp_desc = temp_desc + y[1:len(y)-1]\n",
    "        except:\n",
    "            description_L.append('')\n",
    "            continue\n",
    "\n",
    "        description_L.append(temp_desc)\n",
    "\n",
    "    return description_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To collect this data Office for National Statistics (ONS) asks people in the UK to rate their well-being on an 11-point scale.\n"
     ]
    }
   ],
   "source": [
    "# One sentence from the first of the long descriptions\n",
    "try:\n",
    "    sentence = get_ONS_long_description()[0][522:648]\n",
    "    print(sentence)\n",
    "except:\n",
    "    print(\"There's been a problem\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
