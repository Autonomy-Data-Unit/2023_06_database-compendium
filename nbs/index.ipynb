{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from database_compendium.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from database_compendium.core import *\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Compendium\n",
    "\n",
    "> Collecting, storing, and exploring metadata of datasets from the ONS API, Nomis API, gov.uk, police data API, and nhs.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install database_compendium\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python 3.x\n",
    "- requests library\n",
    "- pandas library\n",
    "- BeautifulSoup library\n",
    "- re (Regular Expression) module\n",
    "- Math\n",
    "- numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows how the functions can be used to collect metadata on the datasets available from each source.  \n",
    "- These scripts are aimed at retrieving dataset titles, two descriptions (long and short), column titles, unique non-numeric column values, and the release date / date of last update.  \n",
    "- Titles and descriptions are given as strings, columns as a list, and unique column values as a dictionary (with the key being the column title).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONS Functions\n",
    "> ONS functions include: get_ONS_datasets_titles_descriptions(), get_ONS_long_description(), get_ONS_datasets_urls(), find_ONS_cols(), find_ONS_cols_and_unique_vals()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Quarterly personal well-being estimates\n",
      "\n",
      "Description:  Seasonally and non seasonally-adjusted quarterly estimates of life satisfaction, feeling that the things done in life are worthwhile, happiness and anxiety in the UK.\n",
      "\n",
      "Long_description:  We are currently reviewing the measures of national well-being and will update this page in summer 2\n",
      "\n",
      "Columns:  ['v4_2', 'LCL', 'UCL', 'yyyy-qq', 'Time', 'uk-only', 'Geography', 'measure-of-wellbeing', 'MeasureOfWellbeing', 'wellbeing-estimate', 'Estimate', 'seasonal-adjustment', 'SeasonalAdjustment']\n",
      "\n",
      "Unique_parameters:  {'v4_2': None, 'LCL': None, 'UCL': None, 'yyyy-qq': ['2017-q2', '2011-q2', '2020-q2', '2016-q4', '2021-q3', '2020-q3', '2014-q3', '2019-q4', '2022-q2', '2021-q1', '2015-q1', '2013-q4', '2022-q4', '2018-q2', '2017-q3', '2020-q1', '2017-q1', '2012-q4', '2021-q4', '2015-q4', '2019-q2', '2022-q1', '2018-q4', '2015-q3', '2014-q2', '2013-q1', '2016-q1', '2022-q3', '2018-q3', '2015-q2', '2019-q1', '2020-q4', '2014-q1', '2014-q4', '2021-q2', '2018-q1', '2013-q3', '2019-q3', '2012-q1', '2011-q3', '2016-q3', '2016-q2', '2012-q3', '2012-q2', '2017-q4', '2013-q2', '2011-q4'], 'Time': ['2017 Q2', '2011 Q2', '2020 Q2', '2016 Q4', '2021 Q3', '2020 Q3', '2014 Q3', '2019 Q4', '2022 Q2', '2021 Q1', '2015 Q1', '2013 Q4', '2022 Q4', '2018 Q2', '2017 Q3', '2020 Q1', '2017 Q1', '2012 Q4', '2021 Q4', '2015 Q4', '2019 Q2', '2022 Q1', '2018 Q4', '2015 Q3', '2014 Q2', '2013 Q1', '2016 Q1', '2022 Q3', '2018 Q3', '2015 Q2', '2019 Q1', '2020 Q4', '2014 Q1', '2014 Q4', '2021 Q2', '2018 Q1', '2013 Q3', '2019 Q3', '2012 Q1', '2011 Q3', '2016 Q3', '2016 Q2', '2012 Q3', '2012 Q2', '2017 Q4', '2013 Q2', '2011 Q4'], 'uk-only': ['K02000001'], 'Geography': ['United Kingdom'], 'measure-of-wellbeing': ['anxiety', 'worthwhile', 'happiness', 'life-satisfaction'], 'MeasureOfWellbeing': ['Anxiety', 'Worthwhile', 'Happiness', 'Life satisfaction'], 'wellbeing-estimate': ['poor', 'very-good', 'good', 'average-mean', 'fair'], 'Estimate': ['Poor', 'Very good', 'Good', 'Average (mean)', 'Fair'], 'seasonal-adjustment': ['non-seasonal-adjustment', 'seasonal-adjustment'], 'SeasonalAdjustment': ['Non-seasonally adjusted', 'Seasonally adjusted']}\n",
      "\n",
      "Latest_release:  2023-05-12T00:00:00.000Z\n"
     ]
    }
   ],
   "source": [
    "from database_compendium.core.ONS_scraper_functions import *\n",
    "\n",
    "# Titles and Descriptions\n",
    "titles, descriptions = get_ONS_datasets_titles_descriptions()\n",
    "\n",
    "# Long Descriptions\n",
    "long_description = get_ONS_long_description()\n",
    "\n",
    "# Dataset URLs which are used to open the dataset and read the column data\n",
    "urls = get_ONS_datasets_urls()\n",
    "\n",
    "# Dataset Columns\n",
    "columns = find_ONS_cols(urls[0])\n",
    "\n",
    "# Dataset Unique Column Values\n",
    "unique_column_vals = find_ONS_cols_and_unique_vals(urls[0])\n",
    "\n",
    "# In this case there isn't a function that gets the date but it can easily be done \n",
    "# using the urls as follows\n",
    "response = requests.get(urls[0], timeout=1)\n",
    "latest_release = str(response.json()['release_date'])\n",
    "\n",
    "# Display the output of each function\n",
    "print('Title: ', titles[0])\n",
    "print('\\nDescription: ', descriptions[0]) \n",
    "print('\\nLong_description: ', long_description[0][:100])\n",
    "print('\\nColumns: ', columns)\n",
    "print('\\nUnique_parameters: ', unique_column_vals)\n",
    "print('\\nLatest_release: ', latest_release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomis Functions\n",
    "> Nomis functions include: get_nomis_datasets_titles_descriptions(), get_nomis_dataset_parameters(), and get_nomis_last_updated()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_compendium.core.Nomis_scraper_functions import *\n",
    "\n",
    "# Titles, Descriptions, and long Descriptions\n",
    "titles, descriptions, l_descriptions = get_nomis_datasets_titles_descriptions()\n",
    "\n",
    "# Unfortunately the Nomis api doesn't currently have a way of collecting columns from a dataset without specifying parameters \n",
    "# before hand. I use a blank array so the data fits in a combined dataframe with the data from other sources.\n",
    "cols = np.empty(len(titles))\n",
    "\n",
    "# Dataset Unique parameters\n",
    "params = get_nomis_datasets_parameters()\n",
    "\n",
    "# Most recent release date\n",
    "latest_release = get_nomis_last_updated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insolvency Functions\n",
    "> The insolvency statistics are released as an excel file once a month, this means most of there are fewer functions as almost all the data needed is in said file. Functions include: get_insolvency_stats(), get_mis_description(), and get_mis_last_updated()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_compendium.core.insolvency_stats_scrapers import *\n",
    "\n",
    "# Insolvency stats are given as a dictionary of dataframes where the key is the dataset title\n",
    "insolvency_stats, long_desc = get_insolvency_stats()\n",
    "titles = list(insolvency_stats.keys())\n",
    "\n",
    "# The descriptions and latest releases are all the same\n",
    "description = get_mis_description()\n",
    "latest_release = get_mis_last_updated()\n",
    "\n",
    "# Dataset columns and unique column values\n",
    "cols = list(insolvency_stats[titles[0]].columns)\n",
    "col_data = get_insolvency_unique_column_vals(insolvency_stats[titles[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Police Data Functions\n",
    "> The police data is a bit more awkward as it requires either a latitude and longitude or a poly-area which is made up of latitude and longitude pairs. In an attempt to make things easier the get_constituency_coordinates() function was added which returns a dictionary containing every westminster parliamentary constituency and four coordinates as a very low res poly-area. For those with no location, the search is done by police force.\n",
    "> Functions include: get_constituency_coordinates(), get_street_level_crimes(), get_crimes_no_loc(), get_searches_no_loc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_compendium.core.police_data_scrapers import *\n",
    "\n",
    "# Coordinates for all constituencies (UK) - for a list of contituency names use constituency_coords.keys()\n",
    "constituency_coords = get_constituency_coordinates()\n",
    "\n",
    "street_level_crimes, sl_last_updated = get_street_level_crimes(constituency_coords['Bethnal Green and Bow'], date='2023-03', loctype='poly')\n",
    "stop_searches, ss_last_updated = get_stop_searches(constituency_coords['Bethnal Green and Bow'], '2023-03', 'poly')\n",
    "\n",
    "no_loc_crimes = get_crimes_no_loc(force='metropolitan', date='2023-03')\n",
    "searches_no_loc = get_searches_no_loc('metropolitan', '2023-03')\n",
    "\n",
    "# Given a dataset gets unique column values - done individually \n",
    "col_data = get_unique_col_vals(street_level_crimes)\n",
    "\n",
    "# Columns are the keys from the unique column values \n",
    "cols = col_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NHS Quality and Outcomes\n",
    "> As with the insolvency stats, this comes as an excel file.\n",
    "> Functions include: get_NHS_qualityOutcomes(), get_qualityOutcomes_uniqueColumnValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_compendium.core.NHS_QualityOutcomes_scrapers import *\n",
    "\n",
    "# Returns dictionary of dataframes.\n",
    "# The latest release and long description are both the same for all datasets in this file\n",
    "NHS_quality_outcomes, long_description, latest_release = get_NHS_qualityOutcomes()\n",
    "\n",
    "# Datasets Titles\n",
    "titles = list(NHS_quality_outcomes.keys())\n",
    "\n",
    "sheet = NHS_quality_outcomes[titles[0]]\n",
    "cols, uniqueParams = get_qualityOutcomes_uniqueColumnValues(sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a combined metadata dataset\n",
    "> This combines all the functions to create a complete dataset containing metadata for every dataset which can then be used to compare datasets in various ways. The prepare_identicalColData() function matches datasets by identical columns and rates the connections based on rarity and the total columns they have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_compendium.core.generate_Metadata import *\n",
    "\n",
    "# Generate metadata\n",
    "metadata_df = createMetadata()\n",
    "\n",
    "# Find connections between dataset using identical columns, format in a way the D3 code can \n",
    "# understand and save (if you want to). Saves to the data file as a json\n",
    "prepare_identicalColData(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>cos_similarity()</b> - takes a dataframe (each row is a vector)(the first column can contain labels), the index of the row you want to compare, and the number of most similar datasets you want to retrieve. Calculates the cosine similarity between the compare row and every other row and the results stored and the top n results are returned.\n",
    "- <b>svm_similarity()</b> - takes the same input as cos_similarity(). Calculates the svm similarity between the compare row and every other row and the results stored and the top n results are returned.\n",
    "- <b>createColsList()</b> - takes the metadata dataframe and creates a list of lists containing the unique column titles for each dataset (titles that are the same with minor punctuational differences are considered equal)\n",
    "- <b>find_identical_cols()</b> - takes the column list created by createColsList() and the index of the dataset you want to compare and returns a dataframe containing the each dataset, the number of columns shared with the dataset you're comparing it with and a list of the shared columns.\n",
    "- <b>scoreConnections()</b> - takes the metadata dataframe, the column list created by createColsList(), then theres also the option to choose an alternative 'alt' method of scoring weight and score 'rare' connections more highly which is set to True by default. This returns a dataframe containing every connectione with the source and target datasets plus the weight of the connection between the two.\n",
    "- <b>formatForD3()</b> - takes a list of titles for the datasets, the scored connections (from scoreConnections function), the cutoff (the minimum strength you want the connections to be (default 0)), whether you want to save the file (True/False) and the name of the saved file.\n",
    "- <b>find_similar_cols()</b> - just like find_identical_cols this function takes the columns list and the index of the compare dataset, however, this purposely ignores identical column names and instead uses fuzzy string matching to find columns which could mean the same thing but are named slightly differently.\n",
    "- <b>plot_network()</b> - generates a network diagram when given a pandas edgelist. Also takes a True/False input which toggles the edges to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scripts interact with the ONS, Nomis, and Police APIs and various web pages. Be mindful of their usage to avoid overloading the servers or violating any terms of use of the APIs.\n",
    "\n",
    "Remember that web scraping can be subject to changes in the website's structure and terms of use. Keep the scripts up-to-date and adapt them as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the provided documentation is a general guide based on the information available in the code snippet. You might need to adjust the documentation according to the specific needs of your project and any further developments made to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the functions and the how the code works check the functions files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
